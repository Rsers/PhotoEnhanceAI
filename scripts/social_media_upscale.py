#!/usr/bin/env python3
"""
ç¤¾äº¤åª’ä½“ç…§ç‰‡è¶…åˆ†è¾¨ç‡å¤„ç†å·¥å…·
ä¸“é—¨ç”¨äºæå‡ç¤¾äº¤åª’ä½“ç…§ç‰‡çš„åˆ†è¾¨ç‡å’Œæ¸…æ™°åº¦

ä½¿ç”¨æ–¹æ³•:
python social_media_upscale.py --input ä½åˆ†è¾¨ç‡å›¾ç‰‡.jpg --output é«˜åˆ†è¾¨ç‡å›¾ç‰‡.jpg

ç‰¹ç‚¹:
- 4å€è¶…åˆ†è¾¨ç‡æ”¾å¤§
- ä¸“é—¨ä¼˜åŒ–ç¤¾äº¤åª’ä½“ç…§ç‰‡å¤„ç†
- æ”¯æŒGPUåŠ é€Ÿ
- å†…å­˜ä¸è¶³æ—¶è‡ªåŠ¨ä½¿ç”¨tileæ¨¡å¼
"""

import argparse
import cv2
import numpy as np
import os
import torch
from models.network_swinir import SwinIR as net
import time
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description='ç¤¾äº¤åª’ä½“ç…§ç‰‡è¶…åˆ†è¾¨ç‡å¤„ç†å·¥å…·')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--output', type=str, help='è¾“å‡ºå›¾ç‰‡è·¯å¾„ï¼ˆé»˜è®¤åœ¨è¾“å…¥æ–‡ä»¶åŒç›®å½•ï¼‰')
    parser.add_argument('--model_path', type=str, 
                       default='model_zoo/swinir/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth', 
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--tile', type=int, default=400, 
                       help='tileå¤§å°ï¼Œæ˜¾å­˜ä¸è¶³æ—¶å‡å°æ­¤å€¼')
    parser.add_argument('--gpu', action='store_true', default=True, 
                       help='ä½¿ç”¨GPUåŠ é€Ÿï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f'âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}')
        return

    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output is None:
        input_path = Path(args.input)
        args.output = str(input_path.parent / f"{input_path.stem}_4x{input_path.suffix}")

    # è®¾å¤‡é…ç½®
    if args.gpu and torch.cuda.is_available():
        device = torch.device('cuda')
        print(f'ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿ: {torch.cuda.get_device_name(0)}')
        print(f'ğŸ“Š æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB')
    else:
        device = torch.device('cpu')
        print('ğŸ’» ä½¿ç”¨CPUå¤„ç†')

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(args.model_path):
        print(f'âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}')
        print('è¯·ç¡®ä¿å·²ä¸‹è½½æ¨¡å‹æ–‡ä»¶')
        return

    print('ğŸ”„ åŠ è½½æ¨¡å‹...')
    start_time = time.time()

    # å®šä¹‰æ¨¡å‹ï¼ˆç»å…¸è¶…åˆ†è¾¨ç‡é…ç½®ï¼‰
    model = net(upscale=4, in_chans=3, img_size=48, window_size=8,
                img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180,
                num_heads=[6, 6, 6, 6, 6, 6], mlp_ratio=2, 
                upsampler='pixelshuffle', resi_connection='1conv')
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    pretrained_model = torch.load(args.model_path, map_location=device, weights_only=False)
    param_key_g = 'params_ema' if 'params_ema' in pretrained_model.keys() else 'params'
    model.load_state_dict(pretrained_model[param_key_g] if param_key_g in pretrained_model.keys() else pretrained_model, strict=True)
    model.eval()
    model = model.to(device)
    
    load_time = time.time() - start_time
    print(f'âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}ç§’)')

    # è¯»å–è¾“å…¥å›¾ç‰‡
    print(f'ğŸ“– è¯»å–å›¾ç‰‡: {args.input}')
    img_lq = cv2.imread(args.input, cv2.IMREAD_COLOR)
    if img_lq is None:
        print('âŒ æ— æ³•è¯»å–å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼')
        return

    original_h, original_w = img_lq.shape[:2]
    print(f'ğŸ“ åŸå§‹å°ºå¯¸: {original_w}x{original_h}')

    # é¢„å¤„ç†
    img_lq = img_lq.astype(np.float32) / 255.
    img_lq = np.transpose(img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # BGR to RGB, HWC to CHW
    img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # NCHW

    # æ¨ç†
    print('ğŸ¯ å¼€å§‹è¶…åˆ†è¾¨ç‡å¤„ç†...')
    inference_start = time.time()
    
    with torch.no_grad():
        # å¡«å……åˆ°8çš„å€æ•°ï¼ˆçª—å£å¤§å°è¦æ±‚ï¼‰
        _, _, h_old, w_old = img_lq.size()
        h_pad = (h_old // 8 + 1) * 8 - h_old
        w_pad = (w_old // 8 + 1) * 8 - w_old
        img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]
        img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]
        
        # é€‰æ‹©æ¨ç†æ¨¡å¼
        if args.tile and (h_old > args.tile or w_old > args.tile):
            print(f'ğŸ”§ ä½¿ç”¨Tileæ¨¡å¼å¤„ç† (tile_size={args.tile})')
            output = tile_inference(img_lq, model, args.tile)
        else:
            print('âš¡ ç›´æ¥æ¨ç†æ¨¡å¼')
            try:
                output = model(img_lq)
            except RuntimeError as e:
                if 'out of memory' in str(e):
                    print('âš ï¸  æ˜¾å­˜ä¸è¶³ï¼Œåˆ‡æ¢åˆ°Tileæ¨¡å¼')
                    torch.cuda.empty_cache()
                    output = tile_inference(img_lq, model, args.tile)
                else:
                    raise e
        
        # è£å‰ªåˆ°åŸå§‹å°ºå¯¸çš„4å€
        output = output[..., :h_old * 4, :w_old * 4]

    inference_time = time.time() - inference_start
    print(f'âœ… å¤„ç†å®Œæˆ ({inference_time:.2f}ç§’)')

    # åå¤„ç†å’Œä¿å­˜
    print('ğŸ’¾ ä¿å­˜ç»“æœ...')
    output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()
    if output.ndim == 3:
        output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HWC-BGR
    output = (output * 255.0).round().astype(np.uint8)

    # ä¿å­˜å›¾ç‰‡
    success = cv2.imwrite(args.output, output)
    if success:
        output_h, output_w = output.shape[:2]
        print(f'ğŸ‰ å¤„ç†æˆåŠŸï¼')
        print(f'ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}')
        print(f'ğŸ“ è¾“å‡ºå°ºå¯¸: {output_w}x{output_h}')
        print(f'ğŸ“ˆ æ”¾å¤§å€æ•°: {output_w//original_w}x')
        print(f'â±ï¸  æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’')
        
        # æ–‡ä»¶å¤§å°ä¿¡æ¯
        input_size = os.path.getsize(args.input) / 1024
        output_size = os.path.getsize(args.output) / 1024
        print(f'ğŸ“¦ æ–‡ä»¶å¤§å°: {input_size:.1f}KB â†’ {output_size:.1f}KB')
    else:
        print('âŒ ä¿å­˜å¤±è´¥')

def tile_inference(img_lq, model, tile_size=400):
    """
    Tileæ¨¡å¼æ¨ç†ï¼Œå°†å¤§å›¾åˆ†å—å¤„ç†ä»¥èŠ‚çœæ˜¾å­˜
    """
    b, c, h, w = img_lq.size()
    tile = min(tile_size, h, w)
    assert tile % 8 == 0, "tile size should be multiple of 8"
    
    tile_overlap = 32
    stride = tile - tile_overlap
    h_idx_list = list(range(0, h-tile, stride)) + [h-tile]
    w_idx_list = list(range(0, w-tile, stride)) + [w-tile]
    E = torch.zeros(b, c, h * 4, w * 4, dtype=img_lq.dtype, device=img_lq.device)
    W = torch.zeros_like(E)

    total_tiles = len(h_idx_list) * len(w_idx_list)
    current_tile = 0

    for h_idx in h_idx_list:
        for w_idx in w_idx_list:
            current_tile += 1
            print(f'  å¤„ç†å— {current_tile}/{total_tiles}', end='\r')
            
            in_patch = img_lq[..., h_idx:h_idx+tile, w_idx:w_idx+tile]
            out_patch = model(in_patch)
            out_patch_mask = torch.ones_like(out_patch)

            E[..., h_idx*4:(h_idx+tile)*4, w_idx*4:(w_idx+tile)*4].add_(out_patch)
            W[..., h_idx*4:(h_idx+tile)*4, w_idx*4:(w_idx+tile)*4].add_(out_patch_mask)
    
    print()  # æ¢è¡Œ
    output = E.div_(W)
    return output

if __name__ == '__main__':
    print("ğŸŒŸ ç¤¾äº¤åª’ä½“ç…§ç‰‡è¶…åˆ†è¾¨ç‡å¤„ç†å·¥å…·")
    print("=" * 50)
    main()
